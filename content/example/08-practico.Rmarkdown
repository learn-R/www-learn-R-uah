---
title: "Regresiones lineales, predictores categóricos y representación gráfica"
linktitle: "8: Regresiones lineales, predictores categóricos y representación gráfica"
date: "2021-08-13"
menu:
  example:
    parent: Ejemplos
    weight: 4
type: docs
toc: true
editor_options: 
  chunk_output_type: console
---

# 0. Objetivo de la práctica

El objetivo del práctico, es avanzar en la etapa de análisis de los datos a través del uso de **regresiones lineales**, para esto usaremos datos previamente procesados de la base de datos a utilizar. Recordemos que estamos en el proceso que nos encontramos del curso y con ello el flujo a utilizar.


![](https://github.com/learn-R/slides/raw/main/img/01/flow-rproject.png)

Entonces, en esta práctica aprenderemos trabajar las __regresiones lineales__, también trabajaremos los __predictores categóricos__, luego veremos las diferencias de la estimación simple con **pesos muestrales**, para continuar con la __extracción de objetos__ y finalmente veremos como __representarlos mediante gráficos y tablas__. 


# 1. Recursos del práctico

En este práctico utilizamos los **datos procesados** de la [**Encuesta Suplementaria de Ingresos (ESI) 2020**](https://www.ine.cl/estadisticas/sociales/ingresos-y-gastos/encuesta-suplementaria-de-ingresos).Recuerden _**siempre**_ consultar el [**libro códigos**](https://www.ine.cl/docs/default-source/encuesta-suplementaria-de-ingresos/bbdd/manual-y-gu%C3%ADa-de-variables/2020/personas-esi-2020.pdf?sfvrsn=f196cb4e_4) antes de trabajar datos.


- [<i class="fas fa-file-archive"></i> `08-class.zip`](https://github.com/learn-R/09-class/raw/main/08-clase.zip)


# 2. Paquetes a utilizar

Para este práctico utilizaremos `srvyr` y `survey`, para estimar los modelos con peso muestral, `sjPlot` para la creación de tablas, `tidyverse` de este universo de paquetes utilizaremos `dplyr` (para la manipulación de datos) y `magrittr` (para utilizar el operador pipe). Recuerden que pueden ver más de las funciones de cada paquetes en la sección de recursos (*Cheatsheets*)


```{r librerias, echo=TRUE, results='hide'}
pacman::p_load(sjPlot, 
               tidyverse, 
               srvyr,
               survey,
               magrittr)
```


# 3. Importar datos

Una vez cargado los paquetes a utilizar, debemos cargar los datos procesados.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

datos <- readRDS(url("https://github.com/learn-R/09-class/raw/main/08-clase/output/data/datos_proc.rds"))

```

```{r, eval = F }
datos <- readRDS("output/data/datos_proc.rds")
```

## Explorar datos

Es relevante explorar los datos que utilizaremos, cómo están previamente procesados ¡no sabemos con que variables estamos trabajando!

```{r names, echo=TRUE, results='markup'}
names(datos)
```


```{r head, echo=TRUE, results='markup'}
head(datos)
```

Ahora sabemos que trabajaremos con seis variables: `"ing_t_t"`, `"est_conyugal"`, `"ciuo08"`, `"sexo"`, `"edad"` y `"fact_cal_esi"`,. Ahora inclusive podemos explorar nuestros datos con `sjPlot::view_df()`

```{r echo=TRUE}
sjPlot::view_df(datos,
                encoding = "UTF-8")
```

Perfecto, podemos ver las variables que tenemos y sus categorías de respuesta pasaremos a la **regresión**

# 5. Modelo de regresión

## 5.1 Formula

Previo a eso recordemos que la fórmula de la regresión lineal simple es:

\begin{equation}
\widehat{Y}=b_{0} +b_{1}X
\end{equation}

Mientras que en la regresión lineal múltiple es:

\begin{equation}
\widehat{Y}=b_{0} +b_{1}X +b_{2}X +b_{x}X
\end{equation}

Donde

- $\widehat{Y}$ es el valor estimado/predicho de $Y$
- $b_{0}$ es el **intercepto** de la recta (el valor de Y cuando X es 0)
- $b_{1}$ y $b_{2}$ son los **coeficiente de regresión**, que nos dice cuánto aumenta Y por cada punto que aumenta X (pendiente)

{{< div "note" >}}

### Ecuaciones en LateX

Previo a revisar regresiones en R, es necesario aprender cómo están hechas estas ecuaciones

Para realizar esto hay dos formas de hacerlo, para ello será muy importante el signo `$`, ¿por qué? Porque hay varias formas de crearlo y en la mayoría se utiliza. Para entenderlo usaremos el ejemplo que está arriba 

Forma 1 de ecuación: Esta forma se utiliza para que la fórmula esté centrada en una linea específica       

```
\begin{equation}
\widehat{Y}=b_{0} +b_{1}X
\end{equation}
```

- Resultado

\begin{equation}
\widehat{Y}=b_{0} +b_{1}X
\end{equation}

- Con 

\begin le indicamos el inicio y con las corcheas le explicamos que queremos escribir una ecuación {equation}
\widehat le especificamos la ecuación
\end le decimos a R que es el fin de la ecuación {equation}


Forma 2: Esta forma se utiliza para que la fórmula este de forma contínua en el texto (_inline_)


```
${Y}=b_{0} +b_{1}X$
```

- Resultado

${Y}=b_{0} +b_{1}X$

Forma 3: Con esta forma también la ecuación se verá centrada

```
$${Y}=b_{0} +b_{1}X$$
```

- Resultado 

$${Y}=b_{0} +b_{1}X$$

Si quieres aprender más sobre el uso de ecuaciones en LateX pueden ir [acá](https://manualdelatex.com/tutoriales/ecuaciones)

{{</div>}}

Les mostramos esto porque de la misma forma se diferencian ambos procedimientos en R

Para la regresión lineal simple se utiliza la siguiente estructura:

```
objeto <- lm(dependiente ~ independiente, data=datos)
```

Mientras que para la regresión lineal múltiple, sólo se añaden más variables


```
objeto <- lm(dependiente ~ independiente1 + independiente 2 + independientex, data=datos)
```

## 5.2 Modelo 

En este práctico no sólo aprenderemos a utilizar regresiones en R, sino que también a utilizalas con pesos muestrales. Pero, ¿cuál es la diferencia? ¡Ahora lo veremos!

### Modelo nulo sin pesos muestrales 

Para ello crearemos en un objeto nuestro modelo nulo

```{r}
modelo0_sin <- lm(ing_t_t ~ 1,
              data = datos)
```
 y lo visualizaremos con 
 
```{r}
summary(modelo0_sin)
```


### Modelo nulo con pesos muestrales

Luego crearemos el modelo con peso muestral, para esto debemos añadir un nuevo argumento `weights`, donde se especifica el factor de expansión 

```{r}
modelo0 <- lm(ing_t_t ~ 1,
              data = datos, 
              weights = fact_cal_esi)
```

Ahora veremos los resultados con 

```{r}
summary(modelo0)
```

Entonces ¿qué diferencias hay?

```{r}
summary(modelo0);summary(modelo0_sin)
```

Entonces, esto en ecuación se vería reflejado de la siguiente forma:

Modelo sin muestral

\begin{equation}
\widehat{Y}= 206355 +b_{1}X
\end{equation}

Modelo con peso muestral

\begin{equation}
\widehat{Y}= 256201 +b_{1}X
\end{equation}

Hay diferencias en el intercepto y en los errores estándar, ya que uno asume pesos de precisión y otro asume pesos muestrales.

Por ello es muy importante incorporar el peso muestral en el análisis, sino los estimadores puntuales no serán válidos y menos la inferencia que se realiza posteriormente, debido a la sobrestimación de la precisión de los estimadores. 


## 5.3 Regresión lineal simple 

Ahora estimaremos modelos simples 

El primero sera entre ingresos y edad (`ing_t_t`, `edad`)

```{r modelo1, echo=TRUE, message=FALSE, warning=FALSE, results='markup'}
modelo1 <- lm(ing_t_t ~ edad,
              data = datos, 
              weights = fact_cal_esi)
```

```{r}
summary(modelo1)
```

Ecuación:

```
\begin{equation}
\widehat{Y}= b_{0} +b_{1}X
\end{equation}

Reemplazamos

\begin{equation}
\widehat{ing_t_t}= 127658.05 + 3511.38*edad
\end{equation}
```

Resultado: 

\begin{equation}
\widehat{ing_t_t}= 127658.05 + 3511.38*edad
\end{equation}


También podemos ver la relación entre ingresos y sexo (`ing_t_t`, `sexo`)

```{r modelo2_sin, echo=TRUE, message=FALSE, warning=FALSE, results='markup'}
modelo2_sin <- lm(ing_t_t ~ sexo,
              data = datos, 
              weights = fact_cal_esi)
```

```{r}
summary(modelo2_sin)
```

¡Pero espera! ¡`sexo` no es una variable continua!

### Predictores categóricos

Previo a esto hay que recordar que `sexo` no es un predictor continuo, y también debemos recordárselo a R. **Este es un proceso que debe realizarse en el código de preparación, pero para ver como procesa R los predictores categóricos en una regresión lo haremos acá**

Para ello utilizamos `forcats` un paquete del universo `tidyverse` con la función `as_factor`

```{r}
datos$sexo <-  forcats::as_factor(datos$sexo) #Recuerden que esto debería ir en el código de preparación
```

Listo ahora veremos el modelo

```{r}
modelo2 <- lm(ing_t_t ~ sexo,
              data = datos, 
              weights = fact_cal_esi)
```

```{r}
summary(modelo2)
```

Como pueden ver ahora especifica qué categoría de respuesta utiliza para la comparación, esto en la variable sexo quizá puede ser intuitivo, pero ¿qué pasaría con las que tienen otras categorías de respuesta?

intentémoslo con `ciuo08` y `est_conyugal`

```{r}
datos$ciuo08 <-  forcats::as_factor(datos$ciuo08) 
datos$est_conyugal <-  forcats::as_factor(datos$est_conyugal)
# recuerden que es para el ejemplo, esto debía hacerse en el procesamiento
```

Modelo para `ciuo08`

```{r}
modelo3 <- lm(ing_t_t ~ ciuo08,
              data = datos, 
              weights = fact_cal_esi)

```

```{r}
summary(modelo3)
```

Modelo para `est_conyugal`

```{r}
modelo4 <- lm(ing_t_t ~ est_conyugal,
              data = datos, 
              weights = fact_cal_esi)
```

```{r}
summary(modelo4)
```


## 5.3  Regresión múltiple

Ahora queremos incorporar las demás variables al modelo, para lo haremos de la siguiente manera ( con +)

```{r regmult, echo=TRUE, message=FALSE, warning=FALSE, results='markup'}
modelo5 <- lm(ing_t_t ~ edad + sexo + ciuo08 + est_conyugal,
              data = datos, 
              weights = fact_cal_esi)
```

```{r}
summary(modelo5)
```

### Regresión con paquete  

[`stats`](https://www.rdocumentation.org/packages/stats/versions/3.6.2)

`stats` tiene su función para hacer diverso tipo de regresiones, esta es `glm`, la cual tiene una estructura similar a la anterior, sólo que esta se debe especificar es tipo de regresión que se necesita en el argumento [`family`](https://www.statmethods.net/advstats/glm.html). Así quedaría nuestro modelo:

```{r}
modelo5_glm <- glm(ing_t_t ~ edad + sexo + ciuo08 + est_conyugal,
               family = gaussian(link = "identity"),
               data = datos, 
               weights = fact_cal_esi)
```

```{r}
summary(modelo5_glm)
```

### Regresión con paquete `survey`

`survey` también tiene su función para regresiones, para ello primero crearemos un objeto encuesta

```{r}
esi_design <- as_survey_design(datos, 
                               ids = 1, 
                               weights = fact_cal_esi)
```

Luego usamos la función `svyglm` y en el argumento `design` dejamos el objeto creado y así  generamos nuestro modelo:

```{r}
modelo5_survey <- svyglm(ing_t_t ~ edad + sexo + ciuo08 + est_conyugal,
                         family = gaussian(link = "identity"),
                         design = esi_design)
```

Ahora visualizamos 

```{r}
summary(modelo5_survey)
```


## 5.4 Información de los modelos

Para conocer la información de los modelos, hay múltiples funciones que podemos utilizar:

La primera función es `str`, la cual nos mostrará su estructura

```{r}
str(modelo5)
```

Luego esta `summary`, la cual hemos estado utilizando a lo largo del práctico 

```{r}
summary(modelo5)
```

¿Pero que pasa si queremos un elemento en particular del modelo?, como este es un objeto, se puede utilizar el símbolo `$` para extraer información de este

```{r}
modelo5$coefficients
modelo5$coefficients[2]
modelo5$coefficients["edad"]
```

¿Qué otros elementos puedo extraer?, para eso `str` nos dirá estructura y elementos que podremos utilizar

```{r}
str(summary(modelo5))
```

```{r}
summary(modelo5)$fstatistic
summary(modelo5)$r.squared
```


Para los valores predichos utilizaremos la función de `sjPlot::get_model_data`


```{r eval=FALSE, include=T}
modelo5$fitted.values
```

```{r}
get_model_data(modelo5, 
               type = c("pred"))
get_model_data(modelo5, 
               type = "pred", 
               terms = "sexo")
```


{{< div "note" >}}
### BONUS

con `broom`, podemos convertir objetos en dataframes, inclusive si lo dejamos a un objeto, podemos imprimirlo con `tab_df` de `sjPlot`

```{r}
print <- broom::augment(modelo5) 
```

```{r eval=FALSE, include=T}
tab_df(print, file = "output/data/modelo.doc")
```
{{</div>}}


Esto nos lleva al último punto del práctico, la visualización 


# 6. Visualización 

Si bien `summary` es un código muy útil para visualizar los modelos creados, el problema es que al observar el objeto creado, no es muy presentable para informes, por eso usaremos la función `tab_model` de `sjPlot`, que tiene la siguiente estructura:

```{r reg-ej, eval=FALSE, message=FALSE, warning=FALSE, include=TRUE}
sjPlot::tab_model(objeto_creado, 
                  show.ci= F/T,  # este argumento muestra los intervalos de confianza
                  show.p = F/T, #Este argumento muestra los valores p
                  show.obs = F/T, # Este argumento muestra las observaciones
                  title = "Título de la tabla a crear",
                  digits = 2, # muestra la cantidad de dígitos que tednrá la tabla
                  p.style = c("numeric", "stars", "numeric_stars", "scientific", "scientific_stars"), #cómo representa el pvalue 
                  encoding = "UTF-8",  # evita errores en caracteres
                  file = "output/figures/reg1_tab.doc") # guarda lo creado automáticamente
```

Para visualizar sólo un modelo puede hacerse de esta forma y en nuestros datos se vería así:

```{r reg-ej-s, echo=TRUE, message=FALSE, warning=FALSE, results='markup'}
sjPlot::tab_model(modelo0, 
                  show.ci=FALSE,  
                  encoding = "UTF-8", 
                  file = "output/figures/regnc_tab.doc")
```

Las ventajas de este código es que se puede especificar el `encoding`, y se puede exportar la tabla, que tiene mejor visualización que la que arroja la consola con `summary`

Pero además se pueden incluir más modelos creados en una sola tabla, para eso usaremos nuevamente la función `tab_model` de `sjPlot` 

```{r message=FALSE, warning=FALSE}
sjPlot::tab_model(list(modelo0, modelo1, modelo2), # los modelos estimados
  show.ci=FALSE, # no mostrar intervalo de confianza (por defecto lo hace)
  p.style = "stars", # asteriscos de significación estadística
  dv.labels = c("Modelo 1", "Modelo 2", "Modelo 3"), # etiquetas de modelos o variables dep.
  string.pred = "Predictores", string.est = "β", # nombre predictores y símbolo beta en tabla
  encoding =  "UTF-8",
  file = "output/figures/reg_tab_all.doc")
```

También se pueden especificar las etiquetas con `dv.labels` y cómo queremos que nos incorpore la significación estadística, con  `p.style`


Para visualizar o graficar los coeficientes de regresión para poder observar el impacto de cada variable en el modelo utilizaremos la función `plot_model` de `sjPlot`, su estructura es la siguiente:

```{r eval=FALSE, include=TRUE}
sjPlot::plot_model(objeto_creado, 
                   ci.lvl = "", #estima el nivel de confianza 
                   title = "",  # es el título
                   show.p = T, # nos muestra los valores p
                   show.values =  T, # nos muestra los valores
                   vline.color = "") # color de la recta vertical
                   

```

Esto visualizado con nuestro modelo se ve así:

```{r message=FALSE, warning=FALSE, , echo=TRUE}
sjPlot::plot_model(modelo5, 
                   show.p = T,
                   show.values =  T,
                   ci.lvl = c(0.95), 
                   title = "Estimación de predictores", 
                   vline.color = "purple")
```


Terminamos por este práctico ¡Pero aún falta la regresión logística!, para eso nos vemos en la próxima clase

# 7. Resumen

En este práctico aprendimos a 

- Crear y visualizar regresiones lineales
- Incorporar predictores categóricos
- Crear y visualizar regresiones múltiples

# 8. Recursos

- [sjPlot](https://strengejacke.github.io/sjPlot/) 
- [tidyverse](https://www.tidyverse.org/packages/)
- [magrittr](https://magrittr.tidyverse.org/)

# 9. Reporte de progreso

¡Recuerda rellenar tu [reporte de progreso](). En tu correo electrónico está disponible el código mediante al cuál debes acceder para actualizar tu estado de avance del curso.